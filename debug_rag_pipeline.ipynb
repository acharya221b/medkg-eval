{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf69dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macharya/dev/medkg-eval/medkg/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and setup complete.\n"
     ]
    }
   ],
   "source": [
    "# debug_rag_pipeline.ipynb\n",
    "\n",
    "# Cell 1: Imports and Initial Setup\n",
    "# ==================================\n",
    "# This cell contains all the necessary imports and loads the environment variables.\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from json_repair import repair_json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from nebula3.gclient.net import ConnectionPool\n",
    "from nebula3.Config import Config\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
    "from bertviz import head_view, model_view\n",
    "import warnings\n",
    "\n",
    "# Configure logging to be clear in the notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
    "\n",
    "# Load environment variables from the .env file in the parent directory\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "print(\"Imports and setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94843b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Define All Helper Functions\n",
    "# =====================================\n",
    "# This cell contains all the stateless helper functions from your RAG pipeline.\n",
    "# You can run this once and then focus on the main logic cell.\n",
    "\n",
    "# --- ROBUST FILE PATHS ---\n",
    "_CURR_DIR = os.getcwd() # Get current working directory of the notebook\n",
    "MODEL_NAME = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "INDEX_FILE = os.path.join(_CURR_DIR, \"../graph_rag/faiss_index.bin\")\n",
    "TEXTS_FILE = os.path.join(_CURR_DIR, \"../graph_rag/semantic_nodes.json\")\n",
    "\n",
    "def connect_nebula():\n",
    "    try:\n",
    "        config = Config()\n",
    "        config.max_connection_pool_size = 10\n",
    "        connection_pool = ConnectionPool()\n",
    "        connection_pool.init([(\"127.0.0.1\", 9669)], config)\n",
    "        client = connection_pool.get_session(\"root\", \"nebula\")\n",
    "        client.execute(\"USE petagraph;\")\n",
    "        logging.info(\"Successfully connected to NebulaGraph.\")\n",
    "        return client, connection_pool\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to NebulaGraph: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_faiss_index():\n",
    "    try:\n",
    "        index = faiss.read_index(INDEX_FILE)\n",
    "        with open(TEXTS_FILE, \"r\") as f: texts = json.load(f)\n",
    "        logging.info(f\"FAISS index loaded successfully from: {INDEX_FILE}\")\n",
    "        return index, texts\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not load FAISS index from '{INDEX_FILE}'. Error: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_prompt_assets(task_name, prompt_id, max_shots, library_dir=\"prompt_library\"):\n",
    "    assets = {\"prompt\": \"\", \"output_format\": \"\", \"shots\": []}\n",
    "    task_dir = os.path.join(library_dir, task_name)\n",
    "    prompts_path = os.path.join(task_dir, \"prompts.json\")\n",
    "    if os.path.exists(prompts_path):\n",
    "        with open(prompts_path, 'r') as f:\n",
    "            prompts = json.load(f).get(\"prompts\", [])\n",
    "            selected = next((p for p in prompts if p.get(\"id\") == prompt_id), None)\n",
    "            if selected: assets.update(selected)\n",
    "    shots_path = os.path.join(task_dir, \"shots.json\")\n",
    "    if os.path.exists(shots_path):\n",
    "        with open(shots_path, 'r') as f:\n",
    "            shots_list = json.load(f).get(\"shots\", [])\n",
    "            loaded_shots = shots_list[0] if shots_list and isinstance(shots_list[0], list) else shots_list\n",
    "            assets[\"shots\"] = loaded_shots[:max_shots]\n",
    "    logging.info(f\"Loaded {len(assets['shots'])} shots for '{task_name}' (max_shots: {max_shots}).\")\n",
    "    return assets\n",
    "\n",
    "# def retrieve_semantic_seeds(query, model, index, texts, top_k=50):\n",
    "#     query_vec = model.encode([query], convert_to_numpy=True)\n",
    "#     _, indices = index.search(query_vec, top_k)\n",
    "#     return [texts[i][\"sui\"] for i in indices[0]]\n",
    "\n",
    "def retrieve_semantic_nodes(query, model, index, texts, top_k=50, top_m=10):\n",
    "    \"\"\"\n",
    "    MODIFIED: This function now retrieves both the top_k SUIs for graph traversal\n",
    "    and the top_m full semantic texts for direct use.\n",
    "    \"\"\"\n",
    "    query_vec = model.encode([query], convert_to_numpy=True)\n",
    "    _, indices = index.search(query_vec, top_k)\n",
    "    \n",
    "    top_indices = indices[0]\n",
    "    \n",
    "    # Get the SUIs for the top_k results (for potential graph traversal)\n",
    "    top_k_suis = [texts[i][\"sui\"] for i in top_indices]\n",
    "    \n",
    "    # --- YOUR NEW FEATURE ---\n",
    "    # Get the full text content for the top_m results directly.\n",
    "    # We use [:top_m] to select the m most similar results from the top_k.\n",
    "    top_m_texts = [texts[i][\"name\"] for i in top_indices[:top_m]]\n",
    "    \n",
    "    logging.info(f\"Retrieved {len(top_k_suis)} SUIs and the top {len(top_m_texts)} semantic texts.\")\n",
    "    return top_k_suis, top_m_texts\n",
    "\n",
    "def get_definitions_from_graph(client, suis):\n",
    "    if not suis: return []\n",
    "    try:\n",
    "        suis_str = \", \".join(f'\"{sui}\"' for sui in suis)\n",
    "        resp_cuis = client.execute(f'GO FROM {suis_str} OVER STY REVERSELY YIELD DISTINCT src(edge) AS cui')\n",
    "        if resp_cuis.is_empty(): return []\n",
    "        cuis = [r.values[0].get_sVal().decode(\"utf-8\") for r in resp_cuis.rows()]\n",
    "        cuis_str = \", \".join(f'\"{cui}\"' for cui in cuis)\n",
    "        resp_defs = client.execute(f'GO FROM {cuis_str} OVER DEF YIELD DISTINCT dst(edge) AS def_id')\n",
    "        if resp_defs.is_empty(): return []\n",
    "        def_ids = [r.values[0].get_sVal().decode(\"utf-8\") for r in resp_defs.rows()]\n",
    "        def_ids_str = \", \".join(f'\"{d}\"' for d in def_ids)\n",
    "        resp_final = client.execute(f'FETCH PROP ON Definition {def_ids_str} YIELD Definition.DEF')\n",
    "        if resp_final.is_empty(): return []\n",
    "        return [r.values[0].get_sVal().decode(\"utf-8\") for r in resp_final.rows()]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error during graph traversal: {e}\")\n",
    "        return []\n",
    "\n",
    "def rerank_definitions(question, definitions, top_k=15):\n",
    "    if not definitions: return []\n",
    "    cross_encoder = CrossEncoder('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "    # The rest of the function works exactly the same, but the results will be much better.\n",
    "    scores = cross_encoder.predict([[question, d] for d in definitions])\n",
    "    scored_definitions = sorted(zip(scores, definitions), key=lambda x: x[0], reverse=True)\n",
    "    top_definitions = [d for _, d in scored_definitions[:top_k]]\n",
    "    logging.info(f\"Re-ranked {len(definitions)} definitions and selected the top {len(top_definitions)}.\")\n",
    "    return top_definitions\n",
    "\n",
    "def format_shots(shots):\n",
    "    if not shots: return \"\"\n",
    "    examples = []\n",
    "    for shot in shots:\n",
    "        inp = shot.get(\"input\", {})\n",
    "        out = shot.get(\"Output\", {})\n",
    "        opts = \"\\\\n\".join([f\"{k}: {v}\" for k, v in inp.get(\"Options\", {}).items()])\n",
    "        example = (f\"---\\nExample Question: {inp.get('Question', '')}\\nExample Options:\\n{opts}\\n\"\n",
    "                   f\"Example Correct Answer:\\n```json\\n{json.dumps(out, indent=2)}\\n```\\n---\")\n",
    "        examples.append(example)\n",
    "    return \"\\\\n\\\\n\".join(examples)\n",
    "\n",
    "def generate_llm_response(llm_client, model_name, question, options, definitions, prompt_assets, consistency_result=\"\", no_rag=False):\n",
    "    \"\"\"\n",
    "    MODIFIED: This function now dynamically builds the prompt based on whether\n",
    "    RAG is enabled, omitting the Context and Guidance sections in no-RAG mode.\n",
    "    \"\"\"\n",
    "    main_prompt_instruction = prompt_assets.get(\"prompt\", \"\")\n",
    "    few_shot_str = format_shots(prompt_assets.get(\"shots\", []))\n",
    "    options_str = \"\\\\n\".join([f\"{k}: {v}\" for k, v in options.items()])\n",
    "    # output_format_instruction = (\n",
    "    # \"You MUST provide your response as a single, valid JSON object with the following keys:\\n\"\n",
    "    # \"1. `cop_index`: The integer index of the correct option.\\n\"\n",
    "    # \"2. `answer`: The full string value of the correct option.\\n\"\n",
    "    # \"3. `why_correct`: This MUST be a list of strings. Each string in the list is a distinct, logical step in your reasoning. You must follow this exact 4-step structure for the list:\\n\"\n",
    "    # \"   - Step 1 (string 1): Quote the exact sentences or phrases from the provided Context that are most relevant to the Question. If there is no Context available, just state 'I don't know'.\\n\"\n",
    "    # \"   - Step 2 (string 2): Analyze and state the key information provided in the Question itself (e.g., specific conditions, patient details).\\n\"\n",
    "    # \"   - Step 3 (string 3): If Context is available, explain how the Context supports the key information in the Question, otherwise state 'I don't know'.\\n\"\n",
    "    # \"   - Step 4 (string 4): Explicitly state why the chosen option is correct based on your reasoning.\\n\"\n",
    "    # \"4. `why_others_incorrect`: A brief explanation for why each of the other options is wrong.\"\n",
    "    # )\n",
    "    # output_format_instruction = (\n",
    "    #     \"You MUST provide your response as a single, valid JSON object with the following keys:\\n\"\n",
    "    #     \"1. `cop_index`: The integer index of the correct option.\\n\"\n",
    "    #     \"2. `answer`: The full string value of the correct option.\\n\"\n",
    "    #     \"3. `why_correct`: A detailed explanation of only the correct answer. This explanation MUST follow a specific three-part structure:\\n\"\n",
    "    #     \"   - First, briefly state the key concepts in the question.\\n\"\n",
    "    #     \"   - Second, quote all the exact sentences from the Context that directly support your answer.\\n\"\n",
    "    #     \"   - Finally, provide a concluding sentence that links the evidence to the chosen answer.\\n\"\n",
    "    #     \"4. `why_others_incorrect`: A brief explanation for why each of the other options is wrong.\"\n",
    "    # )\n",
    "    output_format_instruction = (\"You MUST provide your response as a single, valid JSON object with the keys specified in the output_format. \"\n",
    "                                 \"Ensure the JSON is well-formed and includes all required keys.\")\n",
    "    \n",
    "    # --- DYNAMIC PROMPT COMPONENT LOGIC ---\n",
    "    context_block = \"\"\n",
    "    consistency_guidance = \"\"\n",
    "    if not no_rag:\n",
    "        # Only add context and guidance if RAG is enabled.\n",
    "        context_str = \" \".join(definitions) if definitions else \"No relevant biomedical context found.\"\n",
    "        context_block = f\"Context: {context_str}\\n\\n\"\n",
    "        if consistency_result in [\"CONTRADICTED\", \"NEUTRAL\"]:\n",
    "            consistency_guidance = (\n",
    "                f\"\\n--- CRITICAL GUIDANCE ---\\nA fact-check determined the context is '{consistency_result}' to the question's premise. This strongly indicates the question is flawed or unanswerable. \"\n",
    "                f\"Your primary task is to explain WHY the question is flawed. Set 'cop_index' to the 'None of the above' option if it exists, otherwise set it to -1.\\n--- END GUIDANCE ---\\n\"\n",
    "            )\n",
    "\n",
    "    # Assemble the final prompt from the dynamic components\n",
    "    base_prompt = (\n",
    "        f\"{main_prompt_instruction}\\n\"\n",
    "        f\"output_format: {prompt_assets.get('output_format', '')}\\n\\n\"\n",
    "        f\"{consistency_guidance}\" # Will be empty in no-RAG mode\n",
    "        f\"Examples:\\n{few_shot_str}\\n\\n\"\n",
    "        f\"--- CURRENT TASK ---\\n\"\n",
    "        f\"{context_block}\" # Will be empty in no-RAG mode\n",
    "        f\"Question: {question}\\nOptions:\\n{options_str}\\n\\n\"\n",
    "        f\"Provide your answer. {output_format_instruction}\"\n",
    "    )\n",
    "    for attempt in range(2):\n",
    "        prompt = base_prompt + (\"\\n\\nYour previous response was invalid. Please provide ONLY the JSON object.\" if attempt > 0 else \"\")\n",
    "        try:\n",
    "            print(prompt)\n",
    "            response = llm_client.chat.completions.create(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.0, response_format={\"type\": \"json_object\"})\n",
    "            raw_text = response.choices[0].message.content\n",
    "            # response = llm_client.complete(prompt)\n",
    "            # raw_text = response.text\n",
    "            repaired_json_str = repair_json(raw_text)\n",
    "            \n",
    "            # 2. Parse the now-guaranteed-to-be-valid JSON string.\n",
    "            parsed_json = json.loads(repaired_json_str)\n",
    "\n",
    "            if 'cop_index' not in parsed_json:\n",
    "                raise ValueError(\"Output JSON is missing the required 'cop_index' key.\")\n",
    "            \n",
    "            return parsed_json\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Attempt {attempt + 1} failed: {e}. Raw response: '{locals().get('raw_text', 'N/A')}'\")\n",
    "            time.sleep(1)\n",
    "    logging.error(f\"Failed to get valid LLM response after multiple attempts.\")\n",
    "    return None\n",
    "\n",
    "print(\"All helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22bc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 23:50:08,175 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-08-11 23:50:08,178 - INFO - Load pretrained SentenceTransformer: pritamdeka/S-PubMedBert-MS-MARCO\n",
      "2025-08-11 23:51:13,568 - INFO - FAISS index loaded successfully from: /home/macharya/dev/medkg-eval/../graph_rag/faiss_index.bin\n",
      "2025-08-11 23:51:13,579 - WARNING - Connect 127.0.0.1:9669 failed: socket error connecting to host 127.0.0.1, port 9669 (('127.0.0.1', 9669)): ConnectionRefusedError(111, 'Connection refused')\n",
      "2025-08-11 23:51:13,580 - ERROR - Failed to connect to NebulaGraph: The services status exception: [services: ('127.0.0.1', 9669), status: BAD]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Heavy, Shared Resources\n",
    "# ======================================\n",
    "# This cell loads the FAISS index, Sentence Transformer, and connects to Nebula.\n",
    "# This is a slow cell, so you only run it once per session.\n",
    "\n",
    "st_model = SentenceTransformer(MODEL_NAME)\n",
    "faiss_index, faiss_texts = load_faiss_index()\n",
    "#nebula_client, nebula_pool = connect_nebula()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f588eaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 03:43:34,778 - INFO - Get connection to ('127.0.0.1', 9669)\n",
      "2025-08-12 03:43:34,819 - INFO - Successfully connected to NebulaGraph.\n"
     ]
    }
   ],
   "source": [
    "nebula_client, nebula_pool = connect_nebula()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5063f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All heavy resources loaded and ready.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI client for the LLM\n",
    "llm_client = OpenAI(base_url=os.getenv(\"OPENAI_BASE_URL\"), api_key=os.getenv(\"API_KEY\"))\n",
    "#llm_client= Ollama(model=\"llama3:8b\", request_timeout=300)\n",
    "\n",
    "#response = llm.complete(prompt)\n",
    "#print(response)\n",
    "\n",
    "print(\"All heavy resources loaded and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75a78dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: The Interactive Debugging Cell\n",
    "# =======================================\n",
    "# THIS IS THE MAIN CELL YOU WILL USE FOR DEBUGGING.\n",
    "# 1. Set the parameters for the question you want to debug.\n",
    "# 2. Run this cell to see the output of each stage.\n",
    "# 3. Analyze the output and tune the parameters (e.g., top_k values).\n",
    "# 4. Re-run this cell to see the effect of your changes instantly.\n",
    "\n",
    "# --- 1. SET YOUR DEBUGGING PARAMETERS HERE ---\n",
    "TASK_NAME = 'reasoning_fct'\n",
    "PROMPT_ID = 'v3'\n",
    "MAX_SHOTS = 3\n",
    "MODEL_NAME_TO_DEBUG = 'deepseek-r1:14b' #'llama3.2:latest' #'deepseek-r1:14b'\n",
    "\n",
    "# ID of the specific question you want to analyze (find this in your CSV)\n",
    "QUESTION_ID_TO_DEBUG = \"839de867-3100-4283-a219-ec349eee415f\" #\"0ac6c5c7-9826-441a-81d5-68478e6299bb\" #\"dd57ff3a-0ba0-4e1b-8010-05c9e0629c0c\" \"140d832a-b8ae-4791-aada-6fd62f313adb\" \n",
    "\n",
    "# You can also tune these parameters for experiments\n",
    "RETRIEVAL_TOP_K = 20\n",
    "RERANK_TOP_K = 15\n",
    "\n",
    "# --- 2. LOAD THE SPECIFIC QUESTION DATA ---\n",
    "data_file = f\"data/{TASK_NAME}.csv\"\n",
    "df = pd.read_csv(data_file)\n",
    "question_row = df[df['id'] == QUESTION_ID_TO_DEBUG].iloc[0]\n",
    "\n",
    "question = question_row['question']\n",
    "options = ast.literal_eval(question_row['options'])\n",
    "correct_index = question_row['correct_index']\n",
    "correct_answer_text = question_row['correct_answer']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6940875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DEBUGGING ID: 839de867-3100-4283-a219-ec349eee415f ---\n",
      "Question: Most impoant intracellular buffer ?\n",
      "Options: {'0': 'Bicarbonate', '1': 'Albumin', '2': 'Phosphate', '3': 'Ammonia', 'correct answer': 'Ammonia'}\n",
      "Correct Index: 2 ('Phosphate')\n",
      "--------------------------------------------------\n",
      "\n",
      "--- STAGE 1: Semantic Retrieval (Top 20) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]/home/macharya/dev/medkg-eval/medkg/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 05:43:48,470 - INFO - Retrieved 30000 SUIs and the top 30 semantic texts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "--- STAGE 2: Knowledge Graph Traversal ---\n",
      "Retrieved 793 definitions from the graph.\n",
      "  Initial Def [0]: An abnormal increase in the acidity of the body's fluids...\n",
      "  Initial Def [1]: Catalysis of the reaction: S-methyl-5-thio-D-ribulose 1-phosphate = 5-(methylthio)-2,3-dioxopentyl phosphate + H2O. [EC:...\n",
      "  Initial Def [2]: family of globular proteins found in many plant and animal tissues that tend to bind a wide variety of ligands....\n",
      "  Initial Def [3]: An abnormal phosphate concentration in the urine. [HPO_CONTRIBUTOR:Eurenomics_ewuehl]...\n",
      "  Initial Def [4]: Abnormally low serum sodium levels in the setting of electrolyte/fluid imbalance. This condition may be the result of ex...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- STAGE 3: Re-ranking (Top 15) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pritamdeka/S-PubMedBert-MS-MARCO and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-12 05:43:50,018 - INFO - Use pytorch device: cuda:0\n",
      "Batches: 100%|██████████| 25/25 [00:00<00:00, 27.22it/s]\n",
      "2025-08-12 05:43:51,175 - INFO - Re-ranked 793 definitions and selected the top 15.\n",
      "2025-08-12 05:43:51,178 - INFO - Loaded 3 shots for 'reasoning_fct' (max_shots: 3).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected the top 45 most relevant definitions.\n",
      "  Final Ctx [0]: Intracellular accumulation of the lipid-linked oligosaccharide intermediate Man5GlcNAc2-PP-dolichol. [https://orcid.org/...\n",
      "  Final Ctx [1]: potassium bicarbonate 7.67 MG/ML...\n",
      "  Final Ctx [2]: The directed movement of phosphate ions from the cytosol across the vacuolar membrane and into the vacuolar lumen. [GO_R...\n",
      "  Final Ctx [3]: Ammonium bicarbonate + potassium iodide (product)...\n",
      "  Final Ctx [4]: Ion, Bicarbonate...\n",
      "  Final Ctx [5]: ammonium bicarbonate...\n",
      "  Final Ctx [6]: Catalysis of the reaction: 2 D-glucose 1-phosphate = D-glucose + D-glucose 1,6-bisphosphate. [EC:2.7.1.41, MetaCyc:GLUCO...\n",
      "  Final Ctx [7]: Ions, Bicarbonate...\n",
      "  Final Ctx [8]: Ammonium bicarbonate + ipecacuanha + sodium bicarbonate...\n",
      "  Final Ctx [9]: ammonium phosphate ((NH4)3PO4)...\n",
      "  Final Ctx [10]: Ammonium bicarbonate + ipecacuanha...\n",
      "  Final Ctx [11]: Ammonia + ipecacuanha...\n",
      "  Final Ctx [12]: sodium acid phosphate + sodium + potassium bicarbonate...\n",
      "  Final Ctx [13]: Insulin-like growth factor 2 mRNA-binding protein 3 (579 aa, ~64 kDa) is encoded by the human IGF2BP3 gene. This protein...\n",
      "  Final Ctx [14]: The chemical reactions and pathways involving the ammonium ion. [GOC:dhl, GOC:tb, PMID:14671018]...\n",
      "  Final Ctx [15]: Sodium acid phosphate + sodium bicarbonate...\n",
      "  Final Ctx [16]: Ammonia + ammonium bicarbonate...\n",
      "  Final Ctx [17]: Sodium bicarbonate 2.74% inf...\n",
      "  Final Ctx [18]: Ammonia and ammonium bicarbonate only product...\n",
      "  Final Ctx [19]: Sodium acid phosphate + Sodium bicarbonate...\n",
      "  Final Ctx [20]: A synthetic, colorless or pale yellow liquid that is insoluble in water and miscible with carbon tetrachloride, chlorofo...\n",
      "  Final Ctx [21]: Catalysis of the reaction: GTP + oxaloacetate = GDP + phosphoenolpyruvate + CO2. [EC:4.1.1.32]...\n",
      "  Final Ctx [22]: Immunoassay reagents intended to perform qualitative and/or quantitative analyses of body fluids (typically serum) to de...\n",
      "  Final Ctx [23]: A metallic element that has the atomic symbol Bi, and atomic number 83. Its principal isotope is Bismuth 209....\n",
      "  Final Ctx [24]: Ammonium bicarbonate + potassium iodide...\n",
      "  Final Ctx [25]: Ammonia + Ammonium bicarbonate...\n",
      "  Final Ctx [26]: Ammonia + ammonium bicarbonate (product)...\n",
      "  Final Ctx [27]: Ammonia- and ammonium bicarbonate-containing product...\n",
      "  Final Ctx [28]: U3 small nucleolar ribonucleoprotein protein IMP3 (184 aa, ~22 kDa) is encoded by the human IMP3 gene. This protein play...\n",
      "  Final Ctx [29]: Sodium acid phosphate +sodium bicarbonate...\n",
      "  Final Ctx [30]: Ammonium bicarbonate...\n",
      "  Final Ctx [31]: Catalysis of the reaction: P(1),P(4)-bis(5'-nucleosyl)tetraphosphate + H2O = NTP + NMP. Acts on bis(5'-guanosyl)-, bis(5...\n",
      "  Final Ctx [32]: Ammonia and ammonium bicarbonate product...\n",
      "  Final Ctx [33]: metallic element with atomic symbol Bi, atomic number 83 and atomic weight 208.98; its salts have astringent, antacid an...\n",
      "  Final Ctx [34]: An inherited condition caused by mutation(s) in the ITPA gene, encoding inosine triphosphate pyrophosphatase. It is char...\n",
      "  Final Ctx [35]: Product containing ammonia and ammonium bicarbonate...\n",
      "  Final Ctx [36]: Binding to a phosphorylated amino acid residue within a protein. [GOC:go_curators]...\n",
      "  Final Ctx [37]: Ammonium bicarbonate + Ipecacuanha + Sodium bicarbonate...\n",
      "  Final Ctx [38]: Plasma bicarbonate...\n",
      "  Final Ctx [39]: Insulin-like growth factor-binding protein 1 (259 aa, ~28 kDa) is encoded by the human IGFBP1 gene. This protein is invo...\n",
      "  Final Ctx [40]: Sodium bicarbonate 4.2% inf...\n",
      "  Final Ctx [41]: Ammonium Bicarbonate...\n",
      "  Final Ctx [42]: Product containing only ammonia and ammonium bicarbonate (medicinal product)...\n",
      "  Final Ctx [43]: sodium bicarbonate / sodium phosphate, monobasic...\n",
      "  Final Ctx [44]: Sodium bicarbonate 8.4% inf...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- STAGE 4: LLM Generation ---\n",
      "You are a highly intelligent and accurate medical domain expert and a teacher. You are reviewing a multiple-choice question answers of a medical student. You are given questions, options, and answers provided by the colleague.There is a possibility that the student's answer could be wrong. Review the result and provide a precise and detailed explanation of why the answer is correct or wrong. Additionally, you also provide why the other options are not correct. Ensure that the explanation is detailed and accurate. Don't generate incomplete or incorrect biomedical or clinical information.\n",
      "output_format: Your output format is valid JSON format {'is_answer_correct': 'yes/no', 'cop_index': 'The integer index of the correct option.', 'answer': 'The full string value of the correct option.', 'why_correct': 'A detailed explanation in list format for the correct answer. This list MUST follow a specific three-part structure: 1. Briefly state the key concepts in the question and Context. 2. Quote the exact sentences or phrases from the Context that are most relevant and directly support your answer. 3. Provide a concluding statement that links the evidence to the chosen answer.', 'why_others_incorrect': 'A brief explanation for why each of the other options is wrong.'}\n",
      "\n",
      "Examples:\n",
      "---\n",
      "Example Question: A child has ptosis and poor levator function. What surgery will you do?\n",
      "Example Options:\n",
      "0: Levator muscle resection\\n1: Mullerectomy\\n2: Fasanella Servat surgery\\n3: Frontalis suspension surgery\\ncorrect answer: Frontalis suspension surgery\n",
      "Example Correct Answer:\n",
      "```json\n",
      "{\n",
      "  \"is_answer_correct\": \"yes\",\n",
      "  \"cop_index\": \"3\",\n",
      "  \"correct answer\": \"Frontalis suspension surgery\",\n",
      "  \"why_correct\": [\n",
      "    \"The key concepts in the question and context are the relationship between ptosis, poor levator function, and the appropriate surgical intervention.\",\n",
      "    \"The context states: 'Frontalis suspension surgery is used for severe ptosis with poor levator muscle function.' It also notes that this surgery 'allows the forehead to help elevate the eyelid.'\",\n",
      "    \"Based on the provided context, which explicitly links 'severe ptosis with poor levator muscle function' to 'Frontalis suspension surgery,' this is the correct surgical choice for the described condition.\"\n",
      "  ],\n",
      "  \"why_others_incorrect\": \"The other procedures, such as Levator muscle resection, Mullerectomy, and Fasanella Servat surgery, are used for cases of ptosis with better levator function.\"\n",
      "}\n",
      "```\n",
      "---\\n\\n---\n",
      "Example Question: Temporo-mandibular ligament is attached to:\n",
      "Example Options:\n",
      "0: Lateral aspect of TMJ\\n1: Posterior aspect of TMJ\\n2: Mandibular condyle\\n3: Coronoid process\\ncorrect answer: Lateral aspect of TMJ\n",
      "Example Correct Answer:\n",
      "```json\n",
      "{\n",
      "  \"is_answer_correct\": \"yes\",\n",
      "  \"cop_index\": \"0\",\n",
      "  \"correct answer\": \"Lateral aspect of TMJ\",\n",
      "  \"why_correct\": [\n",
      "    \"The question and context focus on the anatomical attachment point of the temporo-mandibular ligament.\",\n",
      "    \"The context states: 'The temporo-mandibular ligament is attached to the lateral aspect of the Temporo-Mandibular Joint (TMJ).'\",\n",
      "    \"The context directly identifies the 'lateral aspect of the Temporo-Mandibular Joint (TMJ)' as the attachment point for the temporo-mandibular ligament, confirming this as the correct answer.\"\n",
      "  ],\n",
      "  \"why_others_incorrect\": \"The ligament is not attached to the posterior aspect of TMJ, the mandibular condyle, or the coronoid process.\"\n",
      "}\n",
      "```\n",
      "---\\n\\n---\n",
      "Example Question: Lipoproteins are of how many types?\n",
      "Example Options:\n",
      "0: 3\\n1: 2\\n2: 4\\n3: None of the above\\ncorrect answer: 2\n",
      "Example Correct Answer:\n",
      "```json\n",
      "{\n",
      "  \"is_answer_correct\": \"no\",\n",
      "  \"cop_index\": \"3\",\n",
      "  \"correct answer\": \"None of the above\",\n",
      "  \"why_correct\": [\n",
      "    \"The core concept is the classification and number of different types of lipoproteins.\",\n",
      "    \"The context states: 'Lipoproteins are classified into five main types: Chylomicrons, Very Low-Density Lipoproteins (VLDL), Intermediate-Density Lipoproteins (IDL), Low-Density Lipoproteins (LDL), and High-Density Lipoproteins (HDL).'\",\n",
      "    \"The context clearly lists five main types of lipoproteins, which means the options '3', '2', and '4' are all incorrect, making 'None of the above' the correct answer.\"\n",
      "  ],\n",
      "  \"why_others_incorrect\": \"Options '3', '2', and '4' are incorrect as they do not correctly represent the number of main lipoprotein types.\"\n",
      "}\n",
      "```\n",
      "---\n",
      "\n",
      "--- CURRENT TASK ---\n",
      "Context: Intracellular accumulation of the lipid-linked oligosaccharide intermediate Man5GlcNAc2-PP-dolichol. [https://orcid.org/0000-0001-5208-3432, PMID:10581255, PMID:16053906, PMID:28575298] potassium bicarbonate 7.67 MG/ML The directed movement of phosphate ions from the cytosol across the vacuolar membrane and into the vacuolar lumen. [GO_REF:0000078, GOC:TermGenie, PMID:26554016] Ammonium bicarbonate + potassium iodide (product) Ion, Bicarbonate ammonium bicarbonate Catalysis of the reaction: 2 D-glucose 1-phosphate = D-glucose + D-glucose 1,6-bisphosphate. [EC:2.7.1.41, MetaCyc:GLUCOSE-1-PHOSPHATE-PHOSPHODISMUTASE-RXN] Ions, Bicarbonate Ammonium bicarbonate + ipecacuanha + sodium bicarbonate ammonium phosphate ((NH4)3PO4) Ammonium bicarbonate + ipecacuanha Ammonia + ipecacuanha sodium acid phosphate + sodium + potassium bicarbonate Insulin-like growth factor 2 mRNA-binding protein 3 (579 aa, ~64 kDa) is encoded by the human IGF2BP3 gene. This protein is involved in translational regulation, mRNA transport, and the modulation of mRNA half-life. The chemical reactions and pathways involving the ammonium ion. [GOC:dhl, GOC:tb, PMID:14671018] Sodium acid phosphate + sodium bicarbonate Ammonia + ammonium bicarbonate Sodium bicarbonate 2.74% inf Ammonia and ammonium bicarbonate only product Sodium acid phosphate + Sodium bicarbonate A synthetic, colorless or pale yellow liquid that is insoluble in water and miscible with carbon tetrachloride, chloroform and methylene chloride. TRIS-BP is no longer used in the United States. The primary routes of human exposure to TRIS-BP are inhalation, dermal contact and ingestion. Since this compound is no longer produced in the United States, the risk of exposure is low. However, TRIS-BP persists in fabric and plastics, making occupational and consumer exposure possible. Contact with this chemical can irritate the eyes and skin. It is reasonably anticipated to be a human carcinogen. (NCI05) Catalysis of the reaction: GTP + oxaloacetate = GDP + phosphoenolpyruvate + CO2. [EC:4.1.1.32] Immunoassay reagents intended to perform qualitative and/or quantitative analyses of body fluids (typically serum) to determine the protein alpha-1-acid glycoprotein (AAG), also known as orosomucoid. AAG is a plasma protein whose concentration rises significantly during acute inflammatory processes (i.e., it is an acute-phase reactant). Some causes of acute inflammatory responses include surgery, myocardial infarction, infections, and/or tumors. Increased levels of AAG typically occur within 12 hours of the injury and decrease in 4 or 5 days. A metallic element that has the atomic symbol Bi, and atomic number 83. Its principal isotope is Bismuth 209. Ammonium bicarbonate + potassium iodide Ammonia + Ammonium bicarbonate Ammonia + ammonium bicarbonate (product) Ammonia- and ammonium bicarbonate-containing product U3 small nucleolar ribonucleoprotein protein IMP3 (184 aa, ~22 kDa) is encoded by the human IMP3 gene. This protein plays a role in ribosomal RNA processing. Sodium acid phosphate +sodium bicarbonate Ammonium bicarbonate Catalysis of the reaction: P(1),P(4)-bis(5'-nucleosyl)tetraphosphate + H2O = NTP + NMP. Acts on bis(5'-guanosyl)-, bis(5'-xanthosyl)-, bis(5'-adenosyl)- and bis(5'-uridyl)-tetraphosphate. [EC:3.6.1.17, PMID:4955726] Ammonia and ammonium bicarbonate product metallic element with atomic symbol Bi, atomic number 83 and atomic weight 208.98; its salts have astringent, antacid and mildly germicidal properties. An inherited condition caused by mutation(s) in the ITPA gene, encoding inosine triphosphate pyrophosphatase. It is characterized by elevated concentrations of inosine triphosphate in erythrocytes. Product containing ammonia and ammonium bicarbonate Binding to a phosphorylated amino acid residue within a protein. [GOC:go_curators] Ammonium bicarbonate + Ipecacuanha + Sodium bicarbonate Plasma bicarbonate Insulin-like growth factor-binding protein 1 (259 aa, ~28 kDa) is encoded by the human IGFBP1 gene. This protein is involved in cell migration and both insulin-like growth factor binding and signaling. Sodium bicarbonate 4.2% inf Ammonium Bicarbonate Product containing only ammonia and ammonium bicarbonate (medicinal product) sodium bicarbonate / sodium phosphate, monobasic Sodium bicarbonate 8.4% inf\n",
      "\n",
      "Question: Most impoant intracellular buffer ?\n",
      "Options:\n",
      "0: Bicarbonate\\n1: Albumin\\n2: Phosphate\\n3: Ammonia\\ncorrect answer: Ammonia\n",
      "\n",
      "Provide your answer. You MUST provide your response as a single, valid JSON object with the keys specified in the output_format. Ensure the JSON is well-formed and includes all required keys.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 05:43:59,120 - INFO - HTTP Request: POST https://ollama.zib.de/api/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL LLM OUTPUT ---\n",
      "{\n",
      "  \"is_answer_correct\": \"yes\",\n",
      "  \"cop_index\": \"3\",\n",
      "  \"correct answer\": \"Ammonia\",\n",
      "  \"why_correct\": [\n",
      "    \"The question focuses on identifying the primary intracellular buffer system.\",\n",
      "    \"Ammonia functions as a key intracellular buffer by neutralizing acids and forming ammonium ions, which is crucial for maintaining cellular pH stability.\",\n",
      "    \"This makes ammonia the most important intracellular buffer compared to other options like bicarbonate, albumin, or phosphate.\"\n",
      "  ],\n",
      "  \"why_others_incorrect\": [\n",
      "    \"Bicarbonate (0) is a major extracellular buffer and not as significant intracellularly.\",\n",
      "    \"Albumin (1) primarily functions in the extracellular compartment.\",\n",
      "    \"Phosphate (2) has some buffering role but is less predominant than ammonia.\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- ANALYSIS ---\n",
      "Correct Index:    2\n",
      "Predicted Index:  3\n",
      "❌ RESULT: INCORRECT\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- DEBUGGING ID: {QUESTION_ID_TO_DEBUG} ---\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Options: {options}\")\n",
    "print(f\"Correct Index: {correct_index} ('{correct_answer_text}')\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "no_rag_flag=False\n",
    "final_definitions=[]\n",
    "if not no_rag_flag:\n",
    "    # --- 3. RUN THE RAG PIPELINE STEP-BY-STEP ---\n",
    "\n",
    "    # --- STAGE 1: SEMANTIC RETRIEVAL ---\n",
    "    print(f\"--- STAGE 1: Semantic Retrieval (Top {RETRIEVAL_TOP_K}) ---\")\n",
    "    query = question + \" \" + \" \".join(options.values())\n",
    "    # suis = retrieve_semantic_seeds(query, st_model, faiss_index, faiss_texts, top_k=RETRIEVAL_TOP_K)\n",
    "    suis, top_semantic_texts = retrieve_semantic_nodes(query, st_model, faiss_index, faiss_texts, top_k=30000, top_m=30)\n",
    "    # print(f\"Found {len(suis)} semantic seeds (SUIs).\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "    # --- STAGE 2: GRAPH TRAVERSAL ---\n",
    "    print(\"--- STAGE 2: Knowledge Graph Traversal ---\")\n",
    "    retrieved_definitions = get_definitions_from_graph(nebula_client, suis)\n",
    "    print(f\"Retrieved {len(retrieved_definitions)} definitions from the graph.\")\n",
    "    for i, definition in enumerate(retrieved_definitions[:5]): # Print first 5\n",
    "        print(f\"  Initial Def [{i}]: {definition[:120]}...\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    # --- STAGE 3: RE-RANKING ---\n",
    "    print(f\"--- STAGE 3: Re-ranking (Top {RERANK_TOP_K}) ---\")\n",
    "    final_definitions = rerank_definitions(question, retrieved_definitions, top_k=RERANK_TOP_K)\n",
    "    #combined_context = list(set(top_semantic_texts + graph_definitions))\n",
    "    final_definitions = list(set(top_semantic_texts + final_definitions))\n",
    "    #final_definitions = rerank_definitions(question, final_definitions, top_k=RERANK_TOP_K)\n",
    "    print(f\"Selected the top {len(final_definitions)} most relevant definitions.\")\n",
    "    for i, definition in enumerate(final_definitions):\n",
    "        print(f\"  Final Ctx [{i}]: {definition[:120]}...\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "# --- STAGE 4: GENERATION ---\n",
    "print(\"--- STAGE 4: LLM Generation ---\")\n",
    "# Load the prompts and shots for this specific task\n",
    "prompt_assets = load_prompt_assets(TASK_NAME, PROMPT_ID, MAX_SHOTS, library_dir=\"prompt_library\")\n",
    "llm_output = generate_llm_response(llm_client, MODEL_NAME_TO_DEBUG, question, options, final_definitions, prompt_assets, no_rag=no_rag_flag)\n",
    "\n",
    "print(\"\\n--- FINAL LLM OUTPUT ---\")\n",
    "# Pretty-print the JSON output for easy reading\n",
    "if llm_output:\n",
    "    print(json.dumps(llm_output, indent=2))\n",
    "    \n",
    "    # --- AUTOMATED ANALYSIS ---\n",
    "    predicted_index = llm_output.get('cop_index')\n",
    "    print(\"\\n--- ANALYSIS ---\")\n",
    "    print(f\"Correct Index:    {correct_index}\")\n",
    "    print(f\"Predicted Index:  {predicted_index}\")\n",
    "    if str(predicted_index) == str(correct_index):\n",
    "        print(\"✅ RESULT: CORRECT\")\n",
    "    else:\n",
    "        print(\"❌ RESULT: INCORRECT\")\n",
    "else:\n",
    "    print(\"LLM failed to generate a valid response.\")\n",
    "\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662c9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab061edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"cop_index\": 2,\n",
    "  \"answer\": \"Surfactant secretion\",\n",
    "  \"why_correct\": [\n",
    "    \"Step 1: The context provided discusses the role of surfactant in reducing surface tension and its relation to Type II alveolar cells.\",\n",
    "    \"Step 2: The question asks about the cell type responsible for alveolar regeneration and an additional function it performs.\",\n",
    "    \"Step 3: Type II alveolar cells are known for their ability to regenerate the epithelium and secrete surfactant.\",\n",
    "    \"Step 4: Therefore, 'Surfactant secretion' is a correct additional function performed by these cells.\"\n",
    "  ],\n",
    "  \"why_others_incorrect\": [\n",
    "    \"Protease release (0): Not primarily associated with alveolar epithelial cells.\",\n",
    "    \"Phagocytosis (1): This is more related to macrophages, not alveolar cells.\",\n",
    "    \"Recruitment of neutrophils (3): While neutrophils are involved in inflammation, they aren't directly recruited by Type II cells for regeneration.\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, let's tackle this question step by step. The patient is a 56-year-old man with hypertension, diabetes, and a creatinine level of 1.6 mg/dL, which I know indicates moderate renal function since it's above normal but not severely elevated. His pre-treatment blood pressure was 170/100 mmHg, so he definitely has stage 2 hypertension.\n",
    "\n",
    "# Now, the question is about determining the best long-term blood pressure goal for him considering his comorbidities and kidney function. I remember that guidelines often tailor blood pressure targets based on individual risk factors.\n",
    "\n",
    "# Looking at the context provided, there are several key points:\n",
    "# - For patients with diabetes or chronic kidney disease (CKD), especially those without cardiovascular disease, the target is usually below 140/90 mmHg.\n",
    "# - However, in cases where achieving this might be challenging or if it's not well-tolerated, a slightly higher target like 150/80 might be considered. But I need to check the exact wording.\n",
    "\n",
    "# Wait, the context mentions that for patients with prehypertension and comorbid conditions like diabetes, they should aim for less than 130/80 if lifestyle modifications fail. Also, it states that in some cases, especially with certain health issues, higher targets might be set. \n",
    "\n",
    "# But looking again, I see a specific part: \"Advise patients with prehypertensive classification and comorbid conditions (e.g., heart failure, diabetes, kidney disease) to seek appropriate drug therapy if a trial of lifestyle modification fails to reduce BP to 130/80 mm Hg or less.\" This suggests that the target is <130/80 for those with such comorbidities.\n",
    "\n",
    "# However, another part says: \"Measure BP to determine presence of hypertension (e.g., normal, less than 120/80; elevated, 120 to 129/80 or less; hypertension stage 1, 130 to 139/80 to 89; hypertension stage 2, equal or greater than 140/90).\" This is more about categorizing hypertension stages rather than targets.\n",
    "\n",
    "# Wait, I'm a bit confused. Another part mentions: \"Most recent diastolic blood pressure less than 80 mm Hg (HTN, CKD, CAD) (DM)\" and others talk about various BP measurements. But the key is to find the best goal for this patient.\n",
    "\n",
    "# Given his comorbidities (diabetes and CKD), I think the target should be more stringent. The context also mentions that in patients with diabetes and kidney disease, especially if they're on treatment, aiming for <140/90 might be appropriate unless there's high risk of hypotension or other complications.\n",
    "\n",
    "# Wait, but another part says: \"Most recent diastolic blood pressure greater than or equal to 90 mm Hg (HTN, CKD, CAD) (DM)\" which indicates that his current BP is stage 2. But for goals, the context doesn't specify a higher target; instead, it emphasizes the importance of achieving lower targets in these patients.\n",
    "\n",
    "# However, looking at the options provided, option 1 is 150-160/90-95 mmHg, which seems high for someone with diabetes and CKD. The other options are lower: 0 is <130/80, 2 is <140/90, and 3 is <140/85.\n",
    "\n",
    "# I think the correct answer should be option 1 because sometimes in older patients or those with multiple comorbidities, a slightly higher target like 140-160/90 might be considered to avoid excessive lowering which could lead to complications. But wait, the context doesn't explicitly state this. It mainly talks about treating to below certain thresholds.\n",
    "# ...\n",
    "# - Option 3 (140/85) focuses on diastolic but may overlook the importance of systolic control in this context.\n",
    "\n",
    "# **Answer:** {\"cop_index\": \"0\", \"answer\": \"<130/80 mmHg\", \"why_correct\": \"The best target for a patient with hypertension, diabetes, and CKD is <130/80 mmHg to minimize organ damage.\", \"why_others_incorrect\": \"Higher targets are not appropriate due to increased risk of complications.\"}'\n",
    "# 2025-08-12 01:27:39,890 - INFO - HTTP Request: POST https://ollama.zib.de/api/chat/completions \"HTTP/1.1 200 OK\"\n",
    "# Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n",
    "\n",
    "# --- FINAL LLM OUTPUT ---\n",
    "# {\n",
    "#   \"cop_index\": \"0\",\n",
    "#   \"answer\": \"<130/80 mmHg\",\n",
    "#   \"why_correct\": \"The key concepts in the question involve determining the best long-term blood pressure goals for a patient with hypertension, diabetes, and renal function (creatinine 1.6 mg/dL). According to the context provided, 'advising patients with prehypertensive classification and comorbid conditions...to seek appropriate drug therapy if a trial of lifestyle modification fails to reduce BP to 130/80 mmHg or less.' This indicates that the target blood pressure should be <130/80 mmHg for such patients. Additionally, 'most recent diastolic blood pressure less than 80 mm Hg' is noted as a favorable outcome. Therefore, the best goal is to achieve a BP below 130/80 mmHg.\",\n",
    "#   \"why_others_incorrect\": \"Option 1 (150-160/90-95 mmHg) is incorrect because it represents a higher range that does not align with standard treatment goals for patients with comorbid conditions. Option 2 (<140/90 mmHg) and Option 3 (<140/85 mmHg) are also incorrect as they do not reflect the stricter targets recommended for diabetes and kidney disease.\"\n",
    "# }\n",
    "\n",
    "# --- ANALYSIS ---\n",
    "# Correct Index:    0\n",
    "# Predicted Index:  0\n",
    "# ✅ RESULT: CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2540df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- FINAL LLM OUTPUT ---\n",
    "# {\n",
    "#   \"is_answer_correct\": \"no\",\n",
    "#   \"cop_index\": \"1\",\n",
    "#   \"correct answer\": \"150-160/90-95 mmHg\",\n",
    "#   \"reasoning_steps\": [\n",
    "#     \"The context mentions patients with prehypertensive classification and comorbid conditions (e.g., heart failure, diabetes, kidney disease) to seek appropriate drug therapy if a trial of lifestyle modification fails to reduce BP to 130/80 mm Hg or less.\",\n",
    "#     \"The blood pressure goal mentioned in the context is 130/80 mmHg or less for patients with prehypertensive classification and comorbid conditions.\",\n",
    "#     \"However, the patient's pre-treatment blood pressure was 170/100 mmHg, which indicates that the patient has hypertension stage 2 (BP >= 140/90).\",\n",
    "#     \"According to the context, patients with hypertension stage 2 should aim for BP goals of 150-160/90-95 mmHg.\"\n",
    "#   ],\n",
    "#   \"why_correct\": \"The correct answer is 150-160/90-95 mmHg because it corresponds to the BP goal mentioned in the context for patients with hypertension stage 2.\",\n",
    "#   \"why_others_incorrect\": \"Options 0, 2, and 3 are incorrect because they do not correspond to the BP goal mentioned in the context for patients with hypertension stage 2.\"\n",
    "# }\n",
    "\n",
    "# --- ANALYSIS ---\n",
    "# Correct Index:    0\n",
    "# Predicted Index:  1\n",
    "# ❌ RESULT: INCORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6822dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imp sui S18141242\n",
    "#definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "919320d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bio-Specific Cross-Encoder model 'pritamdeka/S-PubMedBert-MS-MARCO'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at pritamdeka/S-PubMedBert-MS-MARCO and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-08-08 04:08:31,067 - INFO - Use pytorch device: cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load a Pre-trained Model and Tokenizer\n",
    "# We'll use a BERT model because its attention is very easy to interpret for this task.\n",
    "# While not the exact LLM you're using, the principles of attention are the same.\n",
    "\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name, output_attentions=True)\n",
    "\n",
    "# print(f\"Loaded '{model_name}' for analysis.\")\n",
    "\n",
    "\n",
    "model_name = \"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    "#model_name = \"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\"\n",
    "\n",
    "print(f\"Loading Bio-Specific Cross-Encoder model '{model_name}'...\")\n",
    "cross_encoder = CrossEncoder(model_name)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8245b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q=\"Which of the following structural elements is characteristic of the ortopramide group drugs?\"\n",
    "# d=[\"The 'ortopramides,' also known as substituted benzamides, represent a significant class of dopamine D2 receptor antagonists. Structurally, the defining characteristic of this group is a benzamide core with a methoxy group positioned at the ortho- (2-) position of the aromatic ring. This specific arrangement is crucial for their pharmacological activity. While some related compounds like phenothiazines are also used as antiemetics, they lack this precise benzamide structure.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd0a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for a set of scores x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x)) # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96bf9d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scoring relevance of each definition against the question... ---\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Scoring relevance of each definition against the question... ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Set show_progress_bar=True to see the progress for a large number of definitions.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m scores = \u001b[43mcross_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- 4. CALCULATE THE PROBABILITIES ---\u001b[39;00m\n\u001b[32m     15\u001b[39m probabilities = softmax(scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/medkg-eval/medkg/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/medkg-eval/medkg/lib/python3.13/site-packages/sentence_transformers/cross_encoder/util.py:68\u001b[39m, in \u001b[36mcross_encoder_predict_rank_args_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m         kwargs.pop(deprecated_arg)\n\u001b[32m     64\u001b[39m         logger.warning(\n\u001b[32m     65\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe CrossEncoder.predict `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` argument is deprecated and has no effect. It will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/medkg-eval/medkg/lib/python3.13/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:628\u001b[39m, in \u001b[36mCrossEncoder.predict\u001b[39m\u001b[34m(self, sentences, batch_size, show_progress_bar, activation_fn, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    592\u001b[39m \u001b[33;03mPerforms predictions with the CrossEncoder on the given sentence pairs.\u001b[39;00m\n\u001b[32m    593\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    625\u001b[39m \u001b[33;03m        # => array([0.6912767, 0.4303499], dtype=float32)\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    627\u001b[39m input_was_singular = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mstr\u001b[39m):  \u001b[38;5;66;03m# Cast an individual pair to a list with length 1\u001b[39;00m\n\u001b[32m    629\u001b[39m     sentences = [sentences]\n\u001b[32m    630\u001b[39m     input_was_singular = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# --- 2. PREPARE THE PAIRS FOR THE CROSS-ENCODER ---\n",
    "# The Cross-Encoder needs a list of [question, context] pairs.\n",
    "pairs = []\n",
    "for definition in definitions:\n",
    "    pairs.append([question, definition])\n",
    "\n",
    "# --- 3. GET THE RELEVANCE SCORES ---\n",
    "# This is the core of the analysis. The model will output a single, meaningful\n",
    "# score for each pair, indicating how relevant the definition is to the question.\n",
    "print(\"\\n--- Scoring relevance of each definition against the question... ---\")\n",
    "# Set show_progress_bar=True to see the progress for a large number of definitions.\n",
    "scores = cross_encoder.predict(pairs, show_progress_bar=False)\n",
    "\n",
    "# --- 4. CALCULATE THE PROBABILITIES ---\n",
    "probabilities = softmax(scores)\n",
    "\n",
    "# --- 5. CREATE A CLEAN, INTERPRETABLE REPORT ---\n",
    "df_scores = pd.DataFrame({\n",
    "    'Relevance_Score (Logit)': scores,\n",
    "    'Probability (%)': [f\"{p:.2%}\" for p in probabilities], # Format as percentage\n",
    "    'Definition': definitions\n",
    "})\n",
    "\n",
    "df_scores.sort_values(by='Relevance_Score (Logit)', ascending=False, inplace=True)\n",
    "df_scores.reset_index(drop=True, inplace=True)\n",
    "print(\"\\n\\n========================================================================\")\n",
    "print(\"  DEFINITIVE RELEVANCE RANKING REPORT\")\n",
    "print(\"========================================================================\")\n",
    "print(\"This table shows which of your retrieved definitions is most relevant to the question.\")\n",
    "print(\"A high positive score is good. A low or negative score is bad.\\n\")\n",
    "\n",
    "# Set display options to show the full text of the definitions\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6dc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"cop_index\": 3,\n",
    "  \"answer\": \"Ammonia\",\n",
    "  \"why_correct\": [\n",
    "    \"Intracellular buffers are typically those found within cells, which play a crucial role in maintaining acid-base balance.\",\n",
    "    \"The context highlights the importance of ammonia and ammonium bicarbonate as intracellular buffers, particularly in clinical chemistry settings.\",\n",
    "    \"Among the options provided, ammonia stands out as an intrinsic component of cellular buffering systems due to its ability to react with hydrogen ions.\",\n",
    "    \"Therefore, considering the emphasis on intracellular buffers, ammonia is the most appropriate choice.\"\n",
    "  ],\n",
    "  \"why_others_incorrect\": [\n",
    "    \"Bicarbonate is primarily extracellular and plays a key role in maintaining pH balance in blood plasma, making it less relevant as an intracellular buffer.\",\n",
    "    \"Albumin, while an important protein in blood, serves more as a carrier protein than a buffer.\",\n",
    "    \"Phosphate can act as a buffer, but its role is not as prominent or intrinsic within cells as ammonia's.\"\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed3105ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK stopwords corpus...\n",
      "Libraries imported and stopwords are ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/macharya/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords corpus...\")\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries imported and stopwords are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "969050c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================\n",
      "  ANALYZING ATTENTION FOR DEFINITION [1 / 1]\n",
      "========================================================================\n",
      "CONTEXT: \"The 'ortopramides,' also known as substituted benzamides, represent a significant class of dopamine D2 receptor antagonists. Structurally, the defining characteristic of this group is a benzamide core with a methoxy group positioned at the ortho- (2-) position of the aromatic ring. This specific arrangement is crucial for their pharmacological activity. While some related compounds like phenothiazines are also used as antiemetics, they lack this precise benzamide structure.\"\n",
      "\n",
      "--- TOP 5 MOST MEANINGFUL ATTENDED-TO WORD PAIRS (from final layer) ---\n",
      "\n",
      "*** WARNING: LOW MEANINGFUL ATTENTION. This context may be irrelevant. ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Question_Token, Context_Token)</th>\n",
       "      <th>Attention_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(gene, structure)</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(protein, compounds)</td>\n",
       "      <td>0.000815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(hem, benz)</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(bones, compounds)</td>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(protein, structure)</td>\n",
       "      <td>0.000731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  (Question_Token, Context_Token)  Attention_Score\n",
       "0               (gene, structure)         0.000970\n",
       "1            (protein, compounds)         0.000815\n",
       "2                     (hem, benz)         0.000745\n",
       "3              (bones, compounds)         0.000736\n",
       "4            (protein, structure)         0.000731"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- END OF ANALYSIS FOR DEFINITION [1] ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: The Definitive Analysis Cell with AGGRESSIVE Filtering\n",
    "# ======================================================================\n",
    "# This version is designed to completely eliminate noise and show only\n",
    "# meaningful semantic connections between non-stop words.\n",
    "\n",
    "# --- 1. PASTE YOUR DATA HERE ---\n",
    "q = \"Which of the following structural elements is characteristic of the ortopramide group drugs?\"\n",
    "\n",
    "d = [\n",
    "    \"The 'ortopramides,' also known as substituted benzamides, represent a significant class of dopamine D2 receptor antagonists. Structurally, the defining characteristic of this group is a benzamide core with a methoxy group positioned at the ortho- (2-) position of the aromatic ring. This specific arrangement is crucial for their pharmacological activity. While some related compounds like phenothiazines are also used as antiemetics, they lack this precise benzamide structure.\"\n",
    "]\n",
    "\n",
    "# --- 2. LOOP THROUGH EACH DEFINITION AND ANALYZE ---\n",
    "for i, context_sentence in enumerate(d):\n",
    "    print(f\"========================================================================\")\n",
    "    print(f\"  ANALYZING ATTENTION FOR DEFINITION [{i+1} / {len(d)}]\")\n",
    "    print(f\"========================================================================\")\n",
    "    print(f\"CONTEXT: \\\"{context_sentence}\\\"\")\n",
    "    \n",
    "    # --- Tokenization and Preparation ---\n",
    "    inputs = tokenizer.encode_plus(question, context_sentence, return_tensors='pt', add_special_tokens=True, truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    # --- Run Model and Get Attention ---\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "    # --- Display the Clean Matrix View ---\n",
    "    #print(f\"\\n--- ATTENTION HEATMAP [{i+1}] ---\")\n",
    "    #display(model_view(attentions, tokens))\n",
    "    \n",
    "    # --- PROGRAMMATIC ANALYSIS WITH AGGRESSIVE FILTERING ---\n",
    "    sep_index = tokens.index('[SEP]')\n",
    "    question_tokens = tokens[1:sep_index]\n",
    "    context_tokens = tokens[sep_index+1:-1]\n",
    "    \n",
    "    attentions_per_layer = torch.stack(attentions).squeeze(1).mean(dim=1)\n",
    "    final_layer_attention = attentions_per_layer[-1]\n",
    "\n",
    "    # --- THE DEFINITIVE FIX FOR THE ANALYSIS ---\n",
    "    # 1. Define what to ignore: special tokens, punctuation, AND stop words.\n",
    "    special_tokens = set(tokenizer.special_tokens_map.values())\n",
    "    punctuation = set(string.punctuation)\n",
    "    # Combine all ignored tokens into one set for efficiency\n",
    "    tokens_to_ignore = special_tokens.union(punctuation).union(stop_words)\n",
    "\n",
    "    attention_scores = []\n",
    "    for q_idx, q_token in enumerate(question_tokens, 1):\n",
    "        # 2. Ignore question tokens that are noise.\n",
    "        if q_token in tokens_to_ignore or q_token.startswith('##'):\n",
    "            continue\n",
    "            \n",
    "        for c_idx, c_token in enumerate(context_tokens, sep_index + 1):\n",
    "            # 3. Ignore context tokens that are noise.\n",
    "            if c_token in tokens_to_ignore or c_token.startswith('##'):\n",
    "                continue\n",
    "\n",
    "            # 4. Ignore \"self-attention\" (optional but good practice)\n",
    "            if q_token == c_token:\n",
    "                continue\n",
    "\n",
    "            score = final_layer_attention[q_idx, c_idx].item()\n",
    "            attention_scores.append(((q_token, c_token), score))\n",
    "\n",
    "    attention_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\n--- TOP 5 MOST MEANINGFUL ATTENDED-TO WORD PAIRS (from final layer) ---\")\n",
    "    df = pd.DataFrame(attention_scores[:5], columns=['(Question_Token, Context_Token)', 'Attention_Score'])\n",
    "    \n",
    "    if df.empty or df['Attention_Score'].iloc[0] < 0.1:\n",
    "        print(\"\\n*** WARNING: LOW MEANINGFUL ATTENTION. This context may be irrelevant. ***\")\n",
    "\n",
    "    display(df)\n",
    "    \n",
    "    print(f\"\\n--- END OF ANALYSIS FOR DEFINITION [{i+1}] ---\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97dd4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(question, definitions, return_tensors='pt', add_special_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids'] # This tells the model which part is the question and which is the context\n",
    "input_ids = inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, context_sentence in enumerate(final_definitions):\n",
    "#     print(f\"\\n--- Analyzing Attention for Definition [{i}] ---\")\n",
    "#     print(f\"Context: {context_sentence}\")\n",
    "    \n",
    "#     # --- Tokenization and Preparation ---\n",
    "#     # This now combines the question with only ONE definition at a time.\n",
    "#     # We also add truncation as a safety measure.\n",
    "#     inputs = tokenizer.encode_plus(\n",
    "#         question, \n",
    "#         context_sentence, \n",
    "#         return_tensors='pt', \n",
    "#         add_special_tokens=True,\n",
    "#         truncation=True, # This will cut off any text that is still too long\n",
    "#         max_length=512   # The model's maximum length\n",
    "#     )\n",
    "    \n",
    "#     token_type_ids = inputs['token_type_ids']\n",
    "#     input_ids = inputs['input_ids']\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "#     # --- Run the Model and Get Attention ---\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, token_type_ids=token_type_ids)\n",
    "#         attentions = outputs.attentions\n",
    "    \n",
    "#     # --- Visualization ---\n",
    "#     # This will render a new interactive heatmap for each definition.\n",
    "#     display(head_view(attentions, tokens))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1248a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: The Analysis Cell - Plug in Your Data Here\n",
    "# ===================================================\n",
    "# 1. Find a question and the exact context from your previous runs' logs.\n",
    "# 2. Paste them into the `question` and `context` variables below.\n",
    "# 3. Run this cell to see the attention heatmap.\n",
    "\n",
    "# --- PASTE YOUR DATA HERE ---\n",
    "question = \"Which of the following structural elements is characteristic of the ortopramide group drugs?\"\n",
    "\n",
    "# Use a small, focused piece of context from your logs for a clear visualization\n",
    "context = \"An orally bioavailable benzamide type inhibitor of histone deacetylase isoenzymes 1, 2, 3 and 10, with potential antineoplastic activity. Tucidinostat is an ortho-halogenated derivative of phenothiazine.\"\n",
    "\n",
    "# --- TOKENIZATION AND PREPARATION ---\n",
    "# This prepares the text in the special format BERT expects: [CLS] question [SEP] context [SEP]\n",
    "inputs = tokenizer.encode_plus(question, context, return_tensors='pt', add_special_tokens=True)\n",
    "token_type_ids = inputs['token_type_ids'] # This tells the model which part is the question and which is the context\n",
    "input_ids = inputs['input_ids']\n",
    "\n",
    "# --- RUN THE MODEL AND GET ATTENTION ---\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, token_type_ids=token_type_ids)\n",
    "    attentions = outputs.attentions # This contains the attention weights for all layers and heads\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "# Convert token IDs back to human-readable tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0]) \n",
    "\n",
    "# Create the visualization\n",
    "# This will render an interactive heatmap directly in your notebook.\n",
    "head_view(attentions, tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medkg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
